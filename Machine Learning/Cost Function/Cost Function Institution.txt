 Visually you can see that the error or difference is equal to the height of this vertical line here when x is equal to 1. Because this lower line is the gap between the actual value of y and the value that the function f predicted, which is a bit further down here. For this first example, when x is 1, f(x) is 0.5. The squared error on the first example is 0.5 minus 1 squared. Remember the cost function, we'll sum over all the training examples in the training set. Let's go on to the second training example. When x is 2, the model is predicting f(x) is 1 and the actual value of y is 2. The error for the second example is equal to the height of this little line segment here, and the squared error is the square of the length of this line segment, so you get 1 minus 2 squared. Let's do the third example. Repeating this process, the error here, also shown by this line segment, is 1.5 minus 3 squared. Next, we sum up all of these terms, which turns out to be equal to 3.5. Then we multiply this term by 1 over 2m, where m is the number of training examples. Since there are three training examples m equals 3, so this is equal to 1 over 2 times 3, where this m here is 3. If we work out the math, this turns out to be 3.5 divided by 6. The cost J is about 0.58. Let's go ahead and plot that over there on the right. Now, let's try one more value for w. How about if w equals 0? What do the graphs for f and J look like when w is equal to 0? It turns out that if w is equal to 0, then f of x is just this horizontal line that is exactly on the x-axis. The error for each example is a line that goes from each point down to the horizontal line that represents f of x equals 0. The cost J when w equals 0 is 1 over 2m times the quantity, 1^2 plus 2^2 plus 3^2, and that's equal to 1 over 6 times 14, which is about 2.33. Let's plot this point where w is 0 and J of 0 is 2.33 over here. You can keep doing this for other values of w. Since w can be any number, it can also be a negative value. If w is negative 0.5, then the line f is a downward-sloping line like this. It turns out that when w is negative 0.5 then you end up with an even higher cost, around 5.25, which is this point up here. You can continue computing the cost function for different values of w and so on and plot these. It turns out that by computing a range of values, you can slowly trace out what the cost function J looks like and that's what J is. To recap, each value of parameter w corresponds to different straight line fit, f of x, on the graph to the left. For the given training set, that choice for a value of w corresponds to a single point on the graph on the right because for each value of w, you can calculate the cost J of w. For example, when w equals 1, this corresponds to this straight line fit through the data and it also corresponds to this point on the graph of J, where w equals 1 and the cost J of 1 equals 0. Whereas when w equals 0.5, this gives you this line which has a smaller slope. This line in combination with the training set corresponds to this point on the cost function graph at w equals 0.5. For each value of w you wind up with a different line and its corresponding costs, J of w, and you can use these points to trace out this plot on the right. Given this, how can you choose the value of w that results in the function f, fitting the data well? Well, as you can imagine, choosing a value of w that causes J of w to be as small as possible seems like a good bet. J is the cost function that measures how big the squared errors are, so choosing w that minimizes these squared errors, makes them as small as possible, will give us a good model. In this example, if you were to choose the value of w that results in the smallest possible value of J of w you'd end up picking w equals 1. As you can see, that's actually a pretty good choice. This results in the line that fits the training data very well. That's how in linear regression you use the cost function to find the value of w that minimizes J. In the more general case where we had parameters w and b rather than just w, you find the values of w and b that minimize J. To summarize, you saw plots of both f and J and worked through how the two are related. As you vary w or vary w and b you end up with different straight lines and when that straight line passes across the data, the cause J is small. The goal of linear regression is to find the parameters w or w and b that results in the smallest possible value for the cost function J. Now here, we worked through our example with a simplified problem using only w.  next , let's visualize what the cost function looks like for the full version of linear regression using both w and b. 
  In the previous video, you saw how to visualize the cost function J of w for a simple linear regression model. In this video, you'll see how to visualize the cost function J of w and b for the full version of linear regression, where you have both w and b as parameters. Let's start with a simple example. Suppose you have a training set with two examples. The first example is x equals 1, y equals 1, and the second example is x equals 2, y equals 2. We want to fit a straight line to this data. In this case, we're going to use the full version of linear regression where we have both w and b as parameters. The hypothesis function f of x is defined as w times x plus b. We want to find the values of w and b that minimize the cost function J of w and b. The cost function J of w and b is defined as 1 over 2m times the sum from i equals 1 to m of f of x i minus y i squared. We're going to visualize this cost function J of w and b. To do this, you'll need a three-dimensional plot. The x-axis will represent the parameter w, the y-axis will represent the parameter b, and the z-axis will represent the cost function J of w and b. Here's what the plot looks like. This is a three-dimensional plot where the x-axis represents the parameter w, the y-axis represents the parameter b, and the z-axis represents the cost function J of w and b. As you can see, this is a three-dimensional plot where the cost function J of w and b is plotted as a function of the parameters w and b. The height of the surface represents the value of the cost function J of w and b. The goal of linear regression is to find the values of w and b that minimize the cost function J of w and b. The values of w and b that minimize J of w and b are found where the surface is at its lowest point. In this example, the lowest point on the surface corresponds to the best values of w and b for the linear regression model. In this case, the best values of w and b are w equals 1 and b equals 0. These values of w and b minimize the cost function J of w and b. This is how you visualize the cost function J of w and b for the