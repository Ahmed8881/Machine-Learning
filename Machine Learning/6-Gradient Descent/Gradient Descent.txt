
Notes on Gradient Descent Algorithm
Objective:

Minimize the cost function 
J(w,b).

General Application:

Gradient descent can minimize any function, not just the cost function for linear regression.
Can be applied to models with multiple parameters (w1, w2, w3, ..., wn).


Algorithm Overview:
1-Initialization
Start with initial guesses for w and 𝑏
often set to 0

2-Iterative Process:
Iteratively adjust w and 𝑏 to reduce the cost 
J(w,b) Continue until  J reaches or nears a minimum.