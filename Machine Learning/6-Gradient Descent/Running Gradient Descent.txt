Notes on Gradient Descent for Linear Regression:
â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢
1-Introduction
â€¢ Objective: Implement gradient descent using the squared error cost function for linear regression to optimize parameters w and ğ‘
â€¢ Outcome: Achieve a linear regression model that fits a straight line to training data.

Components:
1-inear Regression Model: 
â€¢ ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘

2-Squared Error Cost Function:
â€¢ Measures the average of the squares of the errors (differences between predicted and actual values):
â€¢ J(ğ‘¤,ğ‘) = 1/2m âˆ‘(ğ‘¦âˆ’(ğ‘¤ğ‘¥+ğ‘))^2
â€¢ m is the number of training examples

3-Gradient Descent Algorithm:
â€¢ Minimizes the cost function by iteratively moving towards the minimum:
â€¢ Repeat until convergence {
â€¢ ğ‘¤ = ğ‘¤ âˆ’ ğ›¼ âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘¤
â€¢ ğ‘ = ğ‘ âˆ’ ğ›¼ âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘
â€¢ }
â€¢ ğ›¼ is the learning rate

Derivatives Calculation (Optional Slide):
â€¢ Derivatives of the cost function with respect to ğ‘¤ and 
ğ‘:
â€¢ âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘¤ = 1/m âˆ‘(ğ‘¦âˆ’(ğ‘¤ğ‘¥+ğ‘))âˆ—(âˆ’ğ‘¥) 
â€¢ âˆ‚ğ½(ğ‘¤,ğ‘)/âˆ‚ğ‘ = 1/m âˆ‘(ğ‘¦âˆ’(ğ‘¤ğ‘¥+ğ‘))âˆ—(âˆ’1)

