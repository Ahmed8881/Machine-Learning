Notes on Gradient Descent for Linear Regression:
••••••••
1-Introduction
• Objective: Implement gradient descent using the squared error cost function for linear regression to optimize parameters w and 𝑏
• Outcome: Achieve a linear regression model that fits a straight line to training data.

Components:
1-inear Regression Model: 
• 𝑦 = 𝑤𝑥 + 𝑏

2-Squared Error Cost Function:
• Measures the average of the squares of the errors (differences between predicted and actual values):
• J(𝑤,𝑏) = 1/2m ∑(𝑦−(𝑤𝑥+𝑏))^2
• m is the number of training examples

3-Gradient Descent Algorithm:
• Minimizes the cost function by iteratively moving towards the minimum:
• Repeat until convergence {
• 𝑤 = 𝑤 − 𝛼 ∂𝐽(𝑤,𝑏)/∂𝑤
• 𝑏 = 𝑏 − 𝛼 ∂𝐽(𝑤,𝑏)/∂𝑏
• }
• 𝛼 is the learning rate

Derivatives Calculation (Optional Slide):
• Derivatives of the cost function with respect to 𝑤 and 
𝑏:
• ∂𝐽(𝑤,𝑏)/∂𝑤 = 1/m ∑(𝑦−(𝑤𝑥+𝑏))∗(−𝑥) 
• ∂𝐽(𝑤,𝑏)/∂𝑏 = 1/m ∑(𝑦−(𝑤𝑥+𝑏))∗(−1)

