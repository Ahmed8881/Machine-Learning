Notes on Gradient Descent for Linear Regression:
â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢
1-Introduction
â€¢ Objective: Implement gradient descent using the squared error cost function for linear regression to optimize parameters w and ğ‘
â€¢ Outcome: Achieve a linear regression model that fits a straight line to training data.

Components:
1-inear Regression Model: 
â€¢ ğ‘¦ = ğ‘¤ğ‘¥ + ğ‘

2-Squared Error Cost Function:
â€¢ Measures the average of the squares of the errors (differences between predicted and actual values):
â€¢ J(ğ‘¤,ğ‘) = 1/2m âˆ‘(ğ‘¦âˆ’(ğ‘¤ğ‘¥+ğ‘))^2
â€¢ m is the number of training examples